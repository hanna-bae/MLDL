# -*- coding: utf-8 -*-
"""HW_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cm5RvHT5vDbWWD4q7PAQHH5JAKY3-XMZ
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Q5

### (a)
polynomial regression -> predict wage using age. Use cross-vaidation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA?
"""

import pandas as pd
Wage = pd.read_csv('/content/drive/MyDrive/MLDL/Wage.csv')
Wage

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold as KF, cross_val_score
from sklearn.pipeline import Pipeline
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import metrics
from sklearn.feature_selection import f_regression

y = Wage[['wage']]
X = Wage[['age']]

scores = []
models = []
for d in range(0, 15):
  model = Pipeline([('poly', PolynomialFeatures(degree=d)), ('linear', LinearRegression())])
  model.fit(X, y)
  models.append(model)
  score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
  scores.append(-np.mean(score))

x_ = np.arange(0,15)
plt.plot(x_, scores)
plt.ylabel('MSE (CV)')
plt.xlabel('Degrees')
plt.xlim(0,15)
plt.show()

print(np.where(scores == np.min(scores)))

from ISLP.models import (summarize, poly, ModelSpec as MS)
from statsmodels.stats.anova import anova_lm
models = [MS([poly('age', degree=d )]) for d in range(1, 10)]
XEs = [model.fit_transform(Wage) for model in models]
anova_lm(*[sm.OLS(y, X_).fit() for X_ in XEs])

model = Pipeline([('poly', PolynomialFeatures(degree=4)), ('linear', LinearRegression())])
model.fit(X, y)

X_lin = np.linspace(15, 80).reshape(-1,1)
y_lin = model.predict(X_lin)

plt.scatter(X, y, color='orange')
plt.plot(X_lin, y_lin, '-r')

"""###(b)

"""

scores = []
for d in range(1, 15):
  group = pd.cut(Wage['age'], d)
  df_dummies = pd.get_dummies(group)
  X_cv = df_dummies
  model.fit(X_cv, Wage['wage'])
  score = cross_val_score(model, X_cv, Wage['wage'], cv=5, scoring= 'neg_mean_squared_error')
  scores.append(-score)

min_scs =[]
for i in range(len(scores)):
  min_sc = np.mean(scores[i])
  min_scs.append(min_sc)

  print(i+1, min_sc)

"""- Since 11 represents the lowest value, 11 was selected.

"""

groups = pd.cut(Wage['age'], 11)
steps = pd.get_dummies(groups).join(Wage['wage'])
steps

X_ = steps.iloc[:, :-1]
y_ = steps.iloc[:, -1]
regression = sm.GLM(y_, X_).fit()

X_aux = np.linspace(15,80)
groups_aux = pd.cut(X_aux, 11)
aux_dummies = pd.get_dummies(groups_aux)

X_step_lin = np.linspace(15,80)
y_lin = regression.predict(aux_dummies)

plt.scatter(X, y, color='orange')
plt.plot(X_step_lin, y_lin, '-r')

"""# Q6"""

Boston = pd.read_csv('/content/drive/MyDrive/MLDL/Boston.csv')
Boston

"""###(a)

"""

!pip install ISLP

from ISLP.models import bs, ns
from ISLP.pygam import (approx_lam, degrees_of_freedom, plot as plot_gam, anova as anova_gam)
from ISLP.transforms import (BSpline, NaturalSpline)

X = Boston['dis']
y = Boston['nox']

import numpy as np
knots = np.percentile(y, np.linspace(10,90,9))
knots

bs_pop = bs('nox', internal_knots=knots, degree=3)

from ISLP.models import (summarize, poly, ModelSpec as MS)
from statsmodels.stats.anova import anova_lm
design = MS([bs_pop], intercept=False)

Xbs = design.fit_transform(Boston)
summarize(sm.OLS(y, Xbs).fit())

rss = np.zeros(20)
check = -1
min_rss=100
for i in range(1, 21):
  bs_pop = bs('nox', internal_knots=knots, degree=i)
  design = MS([bs_pop], intercept=False)
  poly = design.fit_transform(Boston)
  fit = sm.OLS(y, poly).fit()
  rss[i-1] = sum(fit.resid**2)
  if rss[i-1] < min_rss:
    min_rss = rss[i-1]
    check = i-1

print('min(RSS) : ', min_rss)
print('# of polynomial degrees : ',check)
plt.plot(range(1,21), rss)
plt.scatter(rss.argmin()+1, min(rss), color='r')
plt.xlabel('number of polynomial degrees')
plt.ylabel('RSS')

"""###(b)"""

from sklearn.model_selection import KFold, cross_val_score
check = -1
min_mse=100
lm = LinearRegression()
cv_mse = np.zeros(20)
for i in range(1, 21):
  poly = PolynomialFeatures(i).fit_transform(X.values.reshape(-1,1))
  lm_fit = lm.fit(poly, y)
  kf10 = KFold(n_splits =10, shuffle=True, random_state=19)
  scores = cross_val_score(lm_fit, poly, y, scoring='neg_mean_squared_error', cv=kf10)
  cv_mse[i-1] = np.mean(np.abs(scores))
  if cv_mse[i-1] < min_mse:
    min_mse = cv_mse[i-1]
    check = i-1

print('min(MSE) : ', min_mse)
print('# of polynomial degrees : ',check)
plt.plot(range(1,21), cv_mse)
plt.scatter(cv_mse.argmin()+1, cv_mse.min(), color='r')
plt.xlabel('number of polynomial degrees')
plt.ylabel('10-fold CV MSE')

"""# Q7"""

Carseats = pd.read_csv('/content/drive/MyDrive/MLDL/Carseats.csv')
Carseats

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor, export_graphviz
from sklearn.ensemble import BaggingRegressor, RandomForestRegressor
from sklearn import tree

X = Carseats.drop(columns=['Sales'])
y = Carseats['Sales']
X = pd.get_dummies(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state=19)
tree_ = DecisionTreeRegressor(max_depth=3)
tree_.fit(X_train, y_train )

!pip install six

fig = plt.figure(figsize=(25,20))
_= tree.plot_tree(tree_, feature_names = list(X_train.columns.values), class_names = y.values, filled=True)

from sklearn.metrics import mean_squared_error
print('Test MSE', mean_squared_error(y_test, tree_.predict(X_test)))

"""###(b)"""

depth = []
for i in range(1, 15):
  tree_ = DecisionTreeRegressor(max_depth=i)
  scores = cross_val_score(estimator=tree_, X=X_train, y=y_train, cv=10)
  depth.append(scores.mean())

plt.plot(range(1,15), depth)

from sklearn.tree import export_graphviz
from sklearn.metrics import mean_squared_error
import graphviz

prune_tree = DecisionTreeRegressor(max_depth = depth.index(max(depth))+1)
prune_tree.fit(X_train, y_train)

y_pred = prune_tree.predict(X_test)
mean_squared_error(y_test, y_pred)

data = export_graphviz(prune_tree, out_file=None, feature_names= X_train.columns)
graphviz.Source(data)

"""###(c)"""

from sklearn.svm import SVR
bag = BaggingRegressor()
bag.fit(X_train, y_train)
print('Test MSE', mean_squared_error(y_test, bag.predict(X_test)))

feature_importances = np.mean([tree.feature_importances_ for tree in bag.estimators_], axis=0)
for i in range(len(X_train.columns)):
  print(X_train.columns.values[i], ':', feature_importances[i])
print(max(feature_importances))
rea = pd.Series(feature_importances, index = X_train.columns.values).sort_values(inplace=False)
rea.T.plot(kind='barh', color='r')
plt.xlabel('Importance')
plt.gca().legend_ =None

"""- Price

### (d)
"""

rfs = RandomForestRegressor()
rfs.fit(X_train, y_train)
print('Test MSE', mean_squared_error(y_test, rfs.predict(X_test)))

feature_importances = np.mean([tree.feature_importances_ for tree in rfs.estimators_], axis=0)
for i in range(len(X_train.columns)):
  print(X_train.columns.values[i], ':', feature_importances[i])

rea = pd.Series(feature_importances, index = X_train.columns.values).sort_values(inplace=False)
rea.T.plot(kind='barh', color='r')
plt.xlabel('Importance')
plt.gca().legend_ =None

"""- price

# Q8

###(a)
"""

Hitters = pd.read_csv('/content/drive/MyDrive/MLDL/Hitters.csv')
print(Hitters['Salary'])

Hitters['Salary'].isnull().sum()

Hitters.head()

missing_salary_count = Hitters['Salary'].isnull().sum()
Hitters.dropna(subset=['Salary'], inplace=True)
Hitters['Log_Salary'] = np.log(Hitters['Salary'])
Hitters['Salary'].isnull().sum()

Hitters['League'] = Hitters['League'].map({'A':0, 'N':1})
Hitters['NewLeague'] = Hitters['NewLeague'].map({'A':0, 'N':1})
Hitters['Division'] = Hitters['Division'].map({'W':0, 'E': 1})
Hitters = Hitters.drop('Names', axis=1)

X = Hitters.drop(['Salary', 'Log_Salary'], axis=1)
y = Hitters['Log_Salary']

X_train, X_test = X[:200],X[200:]
y_train, y_test = y[:200], y[200:]

print(len(X_train), len(X_test))

y_train.info()

X_train.head()

X_train.info()

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

lambd = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1]

train_mse = []
test_mse = []

for l in lambd:
    model = GradientBoostingRegressor(n_estimators=1000, learning_rate=l, random_state=22)
    model.fit(X_train, y_train)

    train_pred = model.predict(X_train)
    train_mse.append(mean_squared_error(y_train, train_pred))

    test_pred = model.predict(X_test)
    test_mse.append(mean_squared_error(y_test, test_pred))

plt.figure(1)
plt.plot(lambd, train_mse)
plt.xlabel('lambda')
plt.ylabel('train_mse')
plt.xticks(lambd)

plt.figure(2)
plt.plot(lambd, test_mse)
plt.xlabel('lambda')
plt.ylabel('test_mse')
plt.xticks(lambd)

plt.show()

"""### (b)"""

ml_hitter = LinearRegression().fit(X_train, y_train)
ml_mse = mean_squared_error(y_test, ml_hitter.predict(X_test))

?LassoCV

from sklearn.linear_model import LassoCV, LinearRegression

lasso_hitter = LassoCV(cv=10, random_state=22).fit(X_train, y_train)
lasso_mse = mean_squared_error(y_test, lasso_hitter.predict(X_test))

x_label = np.arange(3)
plt.bar(x_label, [max(test_mse), ml_mse, lasso_mse])
plt.xticks(x_label, ('boost', 'multiple linear regression', 'lasso'))
plt.ylabel('test MSE')

X_train.columns

"""###(c)"""

ld = 0.06
model = GradientBoostingRegressor(n_estimators=1000, learning_rate = ld, random_state=22)
model.fit(X_train, y_train)

feature = model.feature_importances_*100
rea = pd.Series(feature, index=X_train.columns).sort_values(inplace=False)
rea.T.plot(kind='barh', color='r')
plt.xlabel('Importance')
plt.gca().lengend_ = None