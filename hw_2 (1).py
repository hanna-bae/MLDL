# -*- coding: utf-8 -*-
"""HW_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SENo95dDVS_Ms13sQW4evpmTpqLT6qGk
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Q4. Solve ISLP Ch.3, Exercise #10

"""

import pandas as pd
Carseats = pd.read_csv('/content/drive/MyDrive/MLDL/Carseats.csv')
Carseats

"""##(a) Fit a multiple regression model to predict Sales using Price, Urban, and US.

"""

Carseats.info()

Carseats.isna().sum().sort_values(ascending=False)

"""Urban, US - qualitative

Price - quantitative

"""

predictor = Carseats[['Price', 'Urban', 'US']]
predictor

sales = Carseats[['Sales']]
sales

print(list(predictor.columns))

data_dumm = pd.get_dummies(predictor)
print(list(data_dumm))
data_dumm.head(5)

data_dumm['Urban'] = data_dumm['Urban_Yes']
data_dumm['US'] = data_dumm['US_Yes']
data_dumm.drop(['Urban_No', 'Urban_Yes', 'US_No', 'US_Yes'], axis=1, inplace=True)
data_dumm.head(10)

from sklearn.linear_model import LinearRegression
X = data_dumm.values
Y = sales.values
model = LinearRegression()
model.fit(X, Y)
print(model.intercept_, model.coef_, model.score(X,Y))

"""##(b)"""

!pip install statsmodels

import pandas as pd
import statsmodels.api as sm

x = sm.add_constant(data_dumm)
y = sales.values
model = sm.OLS(y, x).fit()
print(model.summary())

"""## (e) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome."""

data_dumm.head(10)

data_dumm.drop(['Urban'],axis=1, inplace=True)

x = sm.add_constant(data_dumm)
y = sales.values
model = sm.OLS(y, x).fit()
print(model.summary())

"""##(g)

"""

cons_co = 13.0308
cons_std = 0.631

price_co = -0.0545
price_std = 0.005

us_co = 1.1996
us_std = 0.258

print('95%% confidence interval for Intercept: [ %2.4f; %2.4f] ' % (cons_co-2*cons_std, cons_co+2*cons_std))
print('95%% confidence interval for Price: [ %2.4f; %2.4f] ' % (price_co-2*price_std, price_co+2*price_std))
print('95%% confidence interval for US: [ %2.4f; %2.4f] ' % (us_co-2*us_std, us_co+2*us_std))

"""##(h)"""

import matplotlib.pyplot as plt
from statsmodels.stats.outliers_influence import OLSInfluence
influence = OLSInfluence(model)
standard_residuals = influence.resid_studentized_internal
plt.figure(figsize=(8,6))
plt.scatter(influence.hat_matrix_diag, standard_residuals, alpha=0.5)
avg = 3/400
plt.axvline(x=avg, color='red', label=f'x = {avg}')
outliers_1 = 3
outliers_2 = -3
plt.axhline(y=outliers_1, label=f'y={outliers_1}' )
plt.axhline(y=outliers_2, label=f'y={outliers_2}' )
plt.xlabel('Leverage')
plt.ylabel('Residuals')
plt.legend()
plt.show()

"""#Q5. Solve ISLP Ch.3, Exercise #14"""

import numpy as np

rng = np.random.default_rng(10)
x1 = rng.uniform(0, 1, size=100)
x2 = 0.5*x1+rng.normal(size=100)/10
y = 2+2*x1+0.3*x2+rng.normal(size=100)

"""##(b)

"""

print(np.corrcoef(x1, x2))
plt.scatter(x1, x2)
plt.xlabel('x1')
plt.ylabel('x2')
plt.show()

"""##(c)"""

X = pd.DataFrame({'x1': x1, 'x2':x2})
X = sm.add_constant(X)
model = sm.OLS(y,X).fit()
print(model.summary())

"""##(d)"""

X = pd.DataFrame({'x1': x1})
X = sm.add_constant(X)
model = sm.OLS(y,X).fit()
print(model.summary())

"""##(e)"""

X = pd.DataFrame({'x2': x2})
X = sm.add_constant(X)
model = sm.OLS(y,X).fit()
print(model.summary())

"""##(g)"""

x1 = np.concatenate([x1, [0.1]])
x2 = np.concatenate([x2, [0.8]])
y = np.concatenate([y, [6]])

X = pd.DataFrame({'x1': x1, 'x2':x2})
X = sm.add_constant(X)
model = sm.OLS(y,X).fit()
print(model.summary())

X = pd.DataFrame({'x1': x1})
X = sm.add_constant(X)
model = sm.OLS(y,X).fit()
print(model.summary())

X = pd.DataFrame({'x2': x2})
X = sm.add_constant(X)
model = sm.OLS(y,X).fit()
print(model.summary())

"""##(h)"""

df = pd.DataFrame({'x1':x1, 'x2':x2, 'y':y})
new = df.iloc[-1:]
origin = df.iloc[:-1]
both = origin.plot(kind='scatter', x='x1', y='x2')
new.plot(ax=both,kind='scatter', x='x1', y='x2', color='red' )

both = origin.plot(kind='scatter', x='x1', y='y')
new.plot(ax=both,kind='scatter', x='x1', y='y', color='red' )

both = origin.plot(kind='scatter', x='x2', y='y')
new.plot(ax=both,kind='scatter', x='x2', y='y', color='red' )

"""#Q9. Solve ISLP Ch.4, Exercise #13

##(a)
"""

import pandas as pd
Weekly = pd.read_csv('/content/drive/MyDrive/MLDL/Weekly.csv')
Weekly

Weekly.describe()

Weekly.info()

import matplotlib as plt
import seaborn as sns

g = sns.PairGrid(Weekly)
g.map_upper(sns.scatterplot)
g.map_lower(sns.kdeplot)
g.map_diag(sns.histplot)

"""##(b) logistic regression with Direction as the response and the five lag plus volume as predictors."""

import pandas as pd
import statsmodels.api as sm
data_dumm = pd.get_dummies(Weekly)
data_dumm['Direction'] = data_dumm['Direction_Up']
x = sm.add_constant(Weekly[['Lag1','Lag2','Lag3','Lag4','Lag5','Volume']])
y = data_dumm['Direction'].values
model = sm.Logit(y,x).fit()
print(model.summary())

"""##(C)"""

from sklearn.metrics import confusion_matrix, accuracy_score
predicted_prob = model.predict(x)
threshold = 0.5
predicted_labels = (predicted_prob > threshold).astype(int)
cm = confusion_matrix(y, predicted_labels)
accuracy = accuracy_score(y, predicted_labels)

print("Confusion Matrix:\n", cm, '\nAccuracy:', accuracy)

"""##(d)"""

train_val = (Weekly['Year'] < 2009)
x_train = sm.add_constant(Weekly.loc[train_val, 'Lag2'])
y_train = (Weekly.loc[train_val, "Direction"] == 'Up').astype(int)
model = sm.Logit(y_train,x_train).fit()
print(model.summary())

x_test = sm.add_constant(Weekly.loc[~train_val, 'Lag2'])
y_test = (Weekly.loc[~train_val, "Direction"] == 'Up').astype(int)
predicted_prob = model.predict(x_test)
threshold = 0.5
predicted_labels = (predicted_prob > threshold).astype(int)
cm = confusion_matrix(y_test, predicted_labels)
accuracy = accuracy_score(y_test, predicted_labels)

print("Confusion Matrix:\n", cm, '\nAccuracy:', accuracy)

"""##(e)"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import confusion_matrix, accuracy_score

x_train = Weekly.loc[train_val, 'Lag2'].to_frame()
y_train = Weekly.loc[train_val, "Direction"]
x_test = Weekly.loc[~train_val, 'Lag2'].to_frame()
y_test = Weekly.loc[~train_val, "Direction"]
label_mapping = {'Down': 0, 'Up': 1}
y_true = y_test.map(label_mapping)
lda = LinearDiscriminantAnalysis()
lda.fit(x_train, y_train)

predicted_prob = lda.predict_proba(x_test)[:, 1]
threshold = 0.5
predicted_labels = (predicted_prob > threshold).astype(int)

cm = confusion_matrix(y_true, predicted_labels)
accuracy = accuracy_score(y_true, predicted_labels)

print("Confusion Matrix:\n", cm, '\nAccuracy:', accuracy)

"""##(f)"""

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
x_train = Weekly.loc[train_val, 'Lag2'].to_frame()
y_train = Weekly.loc[train_val, "Direction"]
x_test = Weekly.loc[~train_val, 'Lag2'].to_frame()
y_test = Weekly.loc[~train_val, "Direction"]
label_mapping = {'Down': 0, 'Up': 1}
y_true = y_test.map(label_mapping)
qda = QuadraticDiscriminantAnalysis()
qda.fit(x_train, y_train)

predicted_prob = qda.predict_proba(x_test)[:, 1]
threshold = 0.5
predicted_labels = (predicted_prob > threshold).astype(int)

cm = confusion_matrix(y_true, predicted_labels)
accuracy = accuracy_score(y_true, predicted_labels)

print("Confusion Matrix:\n", cm, '\nAccuracy:', accuracy)

"""##(g)"""

from sklearn.neighbors import KNeighborsClassifier
x_train = Weekly.loc[train_val, 'Lag2'].to_frame()
y_train = Weekly.loc[train_val, "Direction"]
x_test = Weekly.loc[~train_val, 'Lag2'].to_frame()
y_test = Weekly.loc[~train_val, "Direction"]
np.random.seed(200)
label_mapping = {'Down': 0, 'Up': 1}
y_true = y_test.map(label_mapping)
knn_model = KNeighborsClassifier(n_neighbors=1)
knn_model.fit(x_train, y_train)

predicted_prob = knn_model.predict_proba(x_test)[:, 1]
threshold = 0.5
predicted_labels = (predicted_prob > threshold).astype(int)

cm = confusion_matrix(y_true, predicted_labels)
accuracy = accuracy_score(y_true, predicted_labels)

print("Confusion Matrix:\n", cm, '\nAccuracy:', accuracy)

"""##(h)"""

from sklearn.naive_bayes import GaussianNB
x_train = Weekly.loc[train_val, 'Lag2'].to_frame()
y_train = Weekly.loc[train_val, "Direction"]
x_test = Weekly.loc[~train_val, 'Lag2'].to_frame()
y_test = Weekly.loc[~train_val, "Direction"]

label_mapping = {'Down': 0, 'Up': 1}
y_true = y_test.map(label_mapping)
gnb = GaussianNB()
gnb.fit(x_train, y_train)

predicted_prob = gnb.predict_proba(x_test)[:, 1]
threshold = 0.5
predicted_labels = (predicted_prob > threshold).astype(int)

cm = confusion_matrix(y_true, predicted_labels)
accuracy = accuracy_score(y_true, predicted_labels)

print("Confusion Matrix:\n", cm, '\nAccuracy:', accuracy)

"""#Q10.Exploratory Data Analysis with NYC Taxi Dataset

"""

!pip install pandas pyarrow
import numpy as np
import pandas as pd

# Check the below cells before fill in your code.

import pyarrow.parquet as pq

class Dataset:
  train_x = None  # X (data) of training set.
  train_y = None  # Y (label) of training set.
  test_x = None # X (data) of test set.
  test_y = None # Y (label) of test set.

  def __init__(self):
    self.df_train = pq.read_table('/content/drive/MyDrive/MLDL/yellow_tripdata_2023-01_small_train.parquet').to_pandas()
    self.df_test = pq.read_table('/content/drive/MyDrive/MLDL/yellow_tripdata_2023-01_small_test.parquet').to_pandas()
    self.df_train = self.preprocess_data(self.df_train)
    self.df_test = self.preprocess_data(self.df_test)

  def preprocess_data(self, df):
    ### TODO: Preprocess your data (Impute null values; DO NOT drop any test cases)
    df = df.fillna(0)
    return df

  def getdata_linear_reg(self):
    ### Hint: Use self.df_train, self.df_test
    ### Three features (x): Passenger_count, trip_distance, trip_duration (seconds; use tpep_pickup_datetime, tpep_dropoff_datetime)
    ### Target (y): fare_amount
    self.df_train['trip_duration'] = (self.df_train['tpep_dropoff_datetime'] - self.df_train['tpep_pickup_datetime']).dt.total_seconds()
    self.df_test['trip_duration'] = (self.df_test['tpep_dropoff_datetime'] - self.df_test['tpep_pickup_datetime']).dt.total_seconds()
    self.train_x = self.df_train[['passenger_count','trip_distance', 'trip_duration']]
    self.train_y = self.df_train['fare_amount']
    self.test_x = self.df_test[['passenger_count','trip_distance', 'trip_duration']]
    self.test_y = self.df_test['fare_amount']
    return [self.train_x, self.train_y, self.test_x, self.test_y]

  def correlation(self, X, Y):
    ### TODO: Correlation with each features
    return X.corrwith(Y)

x = pq.read_table('/content/drive/MyDrive/MLDL/yellow_tripdata_2023-01_small_train.parquet').to_pandas()
x['trip_duration'] = (x['tpep_dropoff_datetime'] - x['tpep_pickup_datetime']).dt.total_seconds()

x.head(10)

"""## Test Code

Provide some test code with your Linear regression code. Also, Compare with sklearn.LinearRegression().

### 1. Prepare the dataset (8pt)
Load the train and test datasets.

methods:

(1) **preprocess_data(self, df)**: Handle None and null values (2pt)

(2) **getdata_linear_reg(self)**: Separate the features/labels of the train/test set and save them separately in a numpy array format.(4pt)

(3) **correlation(self, X, Y)**: Calculate the correlation between each features and target Y in training set (2pt)
"""

dataset = Dataset()
[train_x, train_y, test_x, test_y] = dataset.getdata_linear_reg()
print(train_x)
print(train_y)
print(test_x)
print(test_y)

dataset.correlation(train_x, train_y)

"""### Question 2: Linear Regression Using Scikit-learn (6 points)

For this task, you are required to implement linear regression utilizing the scikit-learn library. You will be working with the LinearRegression module, which provides functions and methods tailored for this purpose.

For comprehensive documentation and usage guidelines, please refer to the official scikit-learn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html

Ensure to:
* Properly import the necessary libraries and modules.
* Provide a brief explanation or observation after obtaining the results.
"""

# # Install necessary libraries
# !pip install sklearn

# Import required modules
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

# Initialize the Linear Regression model
linear = LinearRegression()

# Fit the model to the training data (Use train_x, train_y, and linear.fit function)
### TODO: YOUR CODE HERE
linear.fit(train_x, train_y)

# Predict the target values for the test dataset (Use linear.predict function and test_x to find y_hat)
y_hat = linear.predict(test_x)  ### TODO: YOUR CODE HERE

def RMSE(true, predicted):
# Calculate the Root Mean Squared Error (RMSE) for model evaluation
  rmse = np.sqrt(mean_squared_error(test_y, y_hat)) ### TODO: YOUR CODE HERE
  return rmse

error = RMSE(test_y, y_hat)
print(f"Root Mean Squared Error (RMSE) on test data: {error:.2f}")

# Find the slope (coefficients) and intercepts
slope = linear.coef_  ### TODO: YOUR CODE HERE
intercept = linear.intercept_  ### TODO: YOUR CODE HERE
print(f"\nModel Coefficients: {slope}")
print(f"Model Intercept: {intercept:.2f}")

import matplotlib.pyplot as plt
# Add plots to visualize the difference between actual vs predicted values
### TODO: YOUR CODE HERE
plt.scatter(test_y, y_hat, alpha=0.5)
plt.xlabel('Actual')
plt.ylabel('Predicted Values')
plt.show()

"""### 3. Craft Your Linear Regression Algorithm (6pt)
Delve into the foundations of linear regression by constructing your own model to make predictions on the dataset.

While detailed annotations will guide your implementation, do not use external libraries such as scikit-learn. Using the numpy library is permitted.

Begin your exploration by understanding the closed form solution for linear regression. Subsequently, familiarize yourself with the gradient descent methodology.


"""

X_ = train_x
print(X_.shape)
w = np.random.randn(4)
print(w.shape)
print((np.dot(X_, w[1:])+w[0]).shape, len(X_))

import numpy as np
# print(train_y.shape)
# diff = train_y-(np.dot(X_, w[1:])+w[0])
# dw = np.dot(X_.T, diff)
# print(dw.shape, dw)
# w = np.zeros(X_.shape[1]+1)
# print(w)
# for i in range(X_.shape[1]+1):
#   w[i] = np.random.uniform(low=-1, high=1)
# print(w)

from numpy import *

class Linear:

    w = None

    def __init__(self, eta=0.01, epoch=100):
      self.w = None
      self.eta = eta
      self.epoch = epoch

    def setEta(self, etaVal):
      self.eta = etaVal

    def setEpoch(self, nepoch):
      self.epoch = nepoch

    def predict(self, X):
      """
      Perform inference
      """
      ### TODO: YOUR CODE HERE
      if self.w is None:
        raise ValueError("Not trained yet.")
      y_hat = np.dot(X, self.w[1:])+self.w[0]
      return y_hat

    def train(self, X, Y):
      """
      Construct a vanilla linear regressor using gradient descent.
      """
      ### TODO: YOUR CODE HERE
      self.w = np.zeros(X.shape[1]+1)
      threshold = 1.0
      for _ in range(self.epoch):
        y_hat = self.predict(X)
        diff = Y-y_hat
        dw = -(2/len(X))*np.sum(np.dot(X.T, diff))
        db = -(2/len(X))*np.sum(diff)
        if np.isinf(dw) or np.isnan(dw) or np.isinf(db) or np.isnan(db):
            print("Gradient values became infinite or NaN. Stopping training.")
            break
        self.w[1:] = self.w[1:] - self.eta * dw
        self.w[0] = self.w[0] - self.eta * db

      return self.w

def RMSE(y_test, y_hat):
    """
    Return Root Mean Squared Error
    """
    ### TODO : YOUR CODE HERE
    rmse = np.sqrt(np.mean((y_test - y_hat)**2)/len(y_test))
    return rmse

model = Linear()

model.train(train_x, train_y)

y_hat = model.predict(test_x)
print(y_hat)

error = RMSE(test_y, y_hat)
print(error)