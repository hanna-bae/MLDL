# -*- coding: utf-8 -*-
"""kaggle_competition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RggDhkfgPG1RZMET6QJZ2qVJ-hCN0kiF
"""

from google.colab import drive

drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

train = pd.read_csv('/content/drive/MyDrive/MLDL/gist-mldl23f-hw4/train.csv')
test = pd.read_csv('/content/drive/MyDrive/MLDL/gist-mldl23f-hw4/test.csv')

train.info()

train['time_taken'].value_counts()

train.dropna(inplace=True)
train.isnull().sum()

train['day'] = pd.to_datetime(train['date']).dt.day
train['month'] = pd.to_datetime(train['date']).dt.month
train.drop('date', axis=1, inplace=True)
test['day'] = pd.to_datetime(test['date']).dt.day
test['month'] = pd.to_datetime(test['date']).dt.month
test.drop('date', axis=1, inplace=True)

train['dep_hour'] = pd.to_datetime(train['dep_time']).dt.hour
train['dep_m'] = pd.to_datetime(train['dep_time']).dt.minute
train['arr_hour'] = pd.to_datetime(train['arr_time']).dt.hour
train['arr_m'] = pd.to_datetime(train['arr_time']).dt.minute
test['dep_hour'] = pd.to_datetime(test['dep_time']).dt.hour
test['dep_m'] = pd.to_datetime(test['dep_time']).dt.minute
test['arr_hour'] = pd.to_datetime(test['arr_time']).dt.hour
test['arr_m'] = pd.to_datetime(test['arr_time']).dt.minute

train.head()

train.drop('dep_time', axis=1, inplace=True)
train.drop('arr_time', axis=1, inplace=True)
total_time = list(train['time_taken'])
total_hours = []
total_ms = []

for i in range(len(total_time)):
  hours, minutes = total_time[i].split('h')
  total_hours.append(float(hours))
  if 'm' in minutes:
      minutes_val = minutes.rstrip('m')
      split_minutes = minutes_val.split()

      if len(split_minutes) > 0:
          minutes_val = split_minutes[0]
          total_ms.append(float(minutes_val))
      else:
          total_ms.append(0)
  else:
      total_ms.append(0)


train['total_hours'] = total_hours
train['total_ms'] = total_ms
train.drop('time_taken', axis=1, inplace=True)

test.drop('dep_time', axis=1, inplace=True)
test.drop('arr_time', axis=1, inplace=True)
total_time = list(test['time_taken'])
total_hours = []
total_ms = []

for i in range(len(total_time)):
  hours, minutes = total_time[i].split('h')
  total_hours.append(float(hours))
  if 'm' in minutes:
      minutes_val = minutes.rstrip('m')
      split_minutes = minutes_val.split()

      if len(split_minutes) > 0:
          minutes_val = split_minutes[0]
          total_ms.append(float(minutes_val))
      else:
          total_ms.append(0)
  else:
      total_ms.append(0)


test['total_hours'] = total_hours
test['total_ms'] = total_ms
test.drop('time_taken', axis=1, inplace=True)

train.head()

train.drop('id', axis=1, inplace=True)
test.drop('id', axis=1, inplace=True)

train.head()

train['airline'].value_counts()

sns.catplot(y='price', x='airline', data=train.sort_values('price', ascending=False), kind='boxen', height=6, aspect=3)
plt.show()

airline = train[['airline']]
airline = pd.get_dummies(airline, drop_first=True)
airline_t = test[['airline']]
airline_t = pd.get_dummies(airline, drop_first=True)
airline.head()

train['from'].value_counts()

sns.catplot(y='price', x='from', data=train.sort_values('from', ascending=False), kind='boxen', height=4, aspect=3)
plt.show()

from_ = train[['from']]
from_ = pd.get_dummies(from_, drop_first=True)
from_t = test[['from']]
from_t= pd.get_dummies(from_t, drop_first=True)
from_.head()

train['to'].value_counts()

sns.catplot(y='price', x='to', data=train.sort_values('to', ascending=False), kind='boxen', height=4, aspect=3)
plt.show()

to = train[['to']]
to = pd.get_dummies(to, drop_first=True)
to_t = test[['to']]
to_t = pd.get_dummies(to_t, drop_first=True)
to.head()

name= train['stop'].value_counts().index.tolist()
print(name)

sns.catplot(y='price', x='stop', data=train.sort_values('stop', ascending=False), kind='boxen', height=4, aspect=3)
plt.show()

for i in range(len(train['stop'].value_counts())):
  train.replace(name[i], i, inplace=True)

train.head()

name_t= test['stop'].value_counts().index.tolist()
for i in range(len(test['stop'].value_counts())):
  test.replace(name_t[i], i, inplace=True)

test.head()

Train = pd.concat([train, airline, from_, to], axis=1)
Train.drop(['airline', 'from', 'to'], axis=1, inplace=True)

Train.head()

X_test = pd.concat([test, airline_t, from_t, to_t], axis=1)
X_test.drop(['airline', 'from', 'to'], axis=1, inplace=True)

X_test.head()

Train.info()

Train.drop(['ch_code'],axis=1, inplace=True)
Train = pd.get_dummies(Train)

Train.head()

Train.info()

X_test.drop('ch_code', axis=1,inplace=True)
X_test = pd.get_dummies(X_test)

X_test.info()

X = Train.drop('price', axis=1)
y = Train['price']

plt.figure(figsize=(10,10))
sns.heatmap(Train.corr(), annot=True, cmap='RdYlGn')
plt.show()

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state=22)

from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train, y_train)
y_pred = model.predict(X_val)

def MAPE(y_test, y_pred):
  return np.mean(np.abs((y_test-y_pred)/y_test))*100

print(MAPE(y_val, y_pred))

# based on xgboost
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=22)
import xgboost

from sklearn.model_selection import GridSearchCV
param_test1 = {'max_depth' : range(3, 15, 2), 'min_child_weight': range(1,8,2)}

gsearch1 = GridSearchCV(estimator=xgboost.XGBRegressor(learning_rate=0.1, n_estimators=100,
                                                       max_depth=5, min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,
                                                       scale_pos_weight=1, seed=1),
                        param_grid = param_test1, scoring='neg_root_mean_squared_error', n_jobs=1, cv=5)

gsearch1.fit(X_train, y_train)
gsearch1.best_params_

param_test2={
    'gamma':[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7]
}

gsearch2 = GridSearchCV(estimator=xgboost.XGBRegressor(learning_rate=0.04, n_estimators=100, max_depth=13, min_child_weight=1,
                                              gamma=0, subsample=0.8, colsample_bytree=0.8,
                                              objective='reg:squarederror', scale_pos_weight=1, seed=1),
                       param_grid = param_test2, scoring='neg_root_mean_squared_error', n_jobs=3, cv=5)

gsearch2.fit(X_train, y_train)
gsearch2.best_params_

param_test3 = {
    'learning_rate': [0.03,0.15],
    'n_estimators': [77,890]
}

gsearch3 = GridSearchCV(estimator=xgboost.XGBRegressor(learning_rate=0.04, n_estimators=500, max_depth=13, min_child_weight=1,
                                              gamma=0.5, subsample=0.9, colsample_bytree=0.9,
                                              objective='reg:squarederror', scale_pos_weight=1,reg_alpha=50,
                                               reg_lambda=0.05, seed=1),
                       param_grid = param_test3, scoring='neg_root_mean_squared_error', n_jobs=3, cv=5)

gsearch3.fit(X_train, y_train)
gsearch3.best_params_

test4 = {'n_estimators' : [890, 1500]}

gsearch4 = GridSearchCV(estimator=xgboost.XGBRegressor(learning_rate=0.03, n_estimators=500, max_depth=13, min_child_weight=1,
                                              gamma=0.5, subsample=0.9, colsample_bytree=0.9,
                                              objective='reg:squarederror', scale_pos_weight=1,reg_alpha=50,
                                               reg_lambda=0.05, seed=1),
                       param_grid = test4, scoring='neg_root_mean_squared_error', n_jobs=3, cv=5)

gsearch4.fit(X_train, y_train)
gsearch4.best_params_

xgb_model = xgboost.XGBRegressor(learning_rate=0.042, n_estimators=1600, max_depth=13, min_child_weight=1,
                                              gamma=0.5, subsample=0.9, colsample_bytree=0.9,
                                              objective='reg:squarederror', scale_pos_weight=1, seed=1,reg_alpha=50, reg_lambda=0.05
                                 )
xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_val)
print(MAPE(y_val, y_pred))

y_test = xgb_model.predict(X_test)

feature_set = X.columns.values
feature_set

X_train.head()

business = X_train['class_business'].values *50000
X_train['business'] = business
business_val = X_val['class_business'].values *50000
X_val['business'] = business_val
X_train_ = X_train.drop('class_business', axis=1)
X_val_ = X_val.drop('class_business', axis=1)

xgb_model = xgboost.XGBRegressor(learning_rate=0.02, n_estimators=2000, max_depth=13, min_child_weight=1,
                                              gamma=0.5, subsample=0.9, colsample_bytree=0.85,
                                              objective='reg:squarederror', scale_pos_weight=1, seed=1,reg_alpha=50, reg_lambda=0.05
                                 )
xgb_model.fit(X_train_, y_train)
y_pred = xgb_model.predict(X_val_)
print(MAPE(y_val, y_pred))

business_test = X_test['class_business'].values *1000
X_test['business'] = business_test
X_test_ = X_test.drop('class_business', axis=1)
y_test = xgb_model.predict(X_test_)

import csv

predictions_df = pd.DataFrame(y_test, columns=['Predictions'])
predictions_df.to_csv('/content/drive/MyDrive/MLDL/xgbregressor6.csv', index=False)

submission = pd.read_csv('/content/drive/MyDrive/MLDL/gist-mldl23f-hw4/submission.csv')
predictions = pd.read_csv('/content/drive/MyDrive/MLDL/xgbregressor6.csv')
submission['price'] = predictions['Predictions']
predictions_df = pd.DataFrame(submission)
predictions_df.to_csv('/content/drive/MyDrive/MLDL/submission10.csv', index=False)